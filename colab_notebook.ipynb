{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Fine-Tuning Demo Companion Notebook\n",
    "\n",
    "This notebook demonstrates how to load the fine-tuned model, run inference tests, and visualize training performance metrics.\n",
    "\n",
    "It is designed to complement the GRPO fine-tuning script and serves as a guide for further experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "!pip install transformers datasets trl torch wandb\n",
    "\n",
    "# (Optional) Install gradio if you plan to use it for interactive visualizations\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Fine-Tuned Model\n",
    "\n",
    "The following code loads the fine-tuned model and tokenizer. Update `MODEL_NAME_OR_PATH` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Update MODEL_NAME_OR_PATH based on your training output\n",
    "MODEL_NAME_OR_PATH = \"outputs/Qwen-1.5B-GRPO\"  \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_OR_PATH)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME_OR_PATH, torch_dtype=torch.bfloat16).to('cuda')\n",
    "\n",
    "print('Model and tokenizer loaded successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference Tests\n",
    "\n",
    "Use the cell below to interactively input prompts and generate model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "    output = model.generate(**inputs, max_length=256, do_sample=True, top_p=0.95, top_k=50)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example usage\n",
    "prompt = \"<reasoning>What is 7+7?</reasoning><answer>\"\n",
    "print(generate_response(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Monitoring\n",
    "\n",
    "For live performance metrics, consider integrating TensorBoard or Gradio dashboards. \n",
    "\n",
    "For example, you can log training metrics using WandB during training, and visualize them in real-time with WandB's dashboard.\n",
    "\n",
    "Additional code and callbacks can be added to the training script to emit metrics to TensorBoard if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "1. Run the cells sequentially.\n",
    "2. Update `MODEL_NAME_OR_PATH` if necessary based on your model output directory.\n",
    "3. Use the inference cell to experiment with different prompts.\n",
    "4. Refer to the README for additional details on training modifications and performance monitoring enhancements."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
